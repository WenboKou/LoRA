{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes==0.43.0\n",
    "!pip install -U datasets\n",
    "!pip install transformers==4.38.2\n",
    "!pip install peft==0.9.0\n",
    "!pip install sentencepiece==0.1.99\n",
    "!pip install -U accelerate==0.28.0\n",
    "!pip install colorama==0.4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import transformers, datasets\n",
    "from peft import PeftModel\n",
    "from colorama import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import GenerationConfig\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 城市数据\n",
    "with open('city.txt','r',encoding='utf-8') as fp:\n",
    "    city_list=fp.readlines()\n",
    "    city_list=[line.strip().split(' ')[1] for line in city_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_template='''\n",
    "给定一句话，请你按如下步骤处理数据。\n",
    "步骤1：识别这句话中的城市和日期共2个信息\n",
    "步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import time \n",
    "\n",
    "samples = []\n",
    "\n",
    "Q_list=[\n",
    "    ('{city}{year}年{month}月{day}日的天气','%Y-%m-%d'),\n",
    "    ('{city}{year}年{month}月{day}号的天气','%Y-%m-%d'),\n",
    "    ('{city}{month}月{day}日的天气','%m-%d'),\n",
    "    ('{city}{month}月{day}号的天气','%m-%d'),\n",
    "\n",
    "    ('{year}年{month}月{day}日{city}的天气','%Y-%m-%d'),\n",
    "    ('{year}年{month}月{day}号{city}的天气','%Y-%m-%d'),\n",
    "    ('{month}月{day}日{city}的天气','%m-%d'),\n",
    "    ('{month}月{day}号{city}的天气','%m-%d'),\n",
    "\n",
    "    ('你们{year}年{month}月{day}日去{city}玩吗？','%Y-%m-%d'),\n",
    "    ('你们{year}年{month}月{day}号去{city}玩么？','%Y-%m-%d'),\n",
    "    ('你们{month}月{day}日去{city}玩吗？','%m-%d'),\n",
    "    ('你们{month}月{day}号去{city}玩吗？','%m-%d'),\n",
    "]\n",
    "\n",
    "# 生成一批\"1月2号\"、\"1月2日\"、\"2023年1月2号\", \"2023年1月2日\", \"2023-02-02\", \"03-02\"之类的话术, 教会它做日期转换\n",
    "for i in range(1000):\n",
    "    sample = {}\n",
    "    Q=Q_list[random.randint(0,len(Q_list)-1)]\n",
    "    city=city_list[random.randint(0,len(city_list)-1)]\n",
    "    year=random.randint(1990,2025)\n",
    "    month=random.randint(1,12)\n",
    "    day=random.randint(1,28)\n",
    "    time_str='{}-{}-{}'.format(year,month,day)\n",
    "    date_field=time.strftime(Q[1],time.strptime(time_str,'%Y-%m-%d'))\n",
    "    Q=Q[0].format(city=city,year=year,month=month,day=day) # 问题\n",
    "    A=json.dumps({'city':city,'date':date_field},ensure_ascii=False)  # 回答\n",
    "\n",
    "    sample[\"instruction\"] = instruction_template\n",
    "    sample[\"input\"] = Q\n",
    "    sample[\"output\"] = A\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': '\\n给定一句话，请你按如下步骤处理数据。\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n',\n",
       "  'input': '你们1994年4月5号去岷县玩么？',\n",
       "  'output': '{\"city\": \"岷县\", \"date\": \"1994-04-05\"}'},\n",
       " {'instruction': '\\n给定一句话，请你按如下步骤处理数据。\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n',\n",
       "  'input': '你们5月19日去兴化市玩吗？',\n",
       "  'output': '{\"city\": \"兴化市\", \"date\": \"05-19\"}'},\n",
       " {'instruction': '\\n给定一句话，请你按如下步骤处理数据。\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n',\n",
       "  'input': '普宁市10月23日的天气',\n",
       "  'output': '{\"city\": \"普宁市\", \"date\": \"10-23\"}'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data.jsonl', 'w') as f:\n",
    "    for sample in samples:\n",
    "        \n",
    "        f.write(json.dumps(sample) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\", cache_dir= \"./tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(data_point):\n",
    "    prompt = f\"\"\"任务：{data_point[\"instruction\"]}\\n输入：{data_point[\"input\"]}\\n输出：{data_point[\"output\"]}\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=100,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1000 examples [00:00, 217818.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('json', data_files=\"data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5149.02 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[\"train\"].shuffle().map(generate_training_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = data.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "train_data = train_test_data[\"train\"]\n",
    "test_data = train_test_data[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./cache\"\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 從指定的模型名稱或路徑載入預訓練的語言模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    quantization_config=nf4_config,\n",
    "    low_cpu_mem_usage = True\n",
    ")\n",
    "\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "# 設定模型推理時需要用到的decoding parameters\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    num_beams=1,\n",
    "    top_p=0.3,\n",
    "    no_repeat_ngram_size=3,\n",
    "    pad_token_id=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive\"  # 設定作業結果輸出目錄 (如果想要把作業結果存在其他目錄底下可以修改這裡，強烈建議存在預設值的子目錄下，也就是Google Drive裡)\n",
    "ckpt_dir = \"./exp1\" # 設定model checkpoint儲存目錄 (如果想要將model checkpoints存在其他目錄下可以修改這裡)\n",
    "num_epoch = 1  # 設定訓練的總Epoch數 (數字越高，訓練越久，若使用免費版的colab需要注意訓練太久可能會斷線)\n",
    "LEARNING_RATE = 3e-4  # 設定學習率\n",
    "\n",
    "cache_dir = \"./cache\"  # 設定快取目錄路徑\n",
    "from_ckpt = False  # 是否從checkpoint載入模型的權重，預設為否\n",
    "ckpt_name = None  # 從特定checkpoint載入權重時使用的檔案名稱，預設為無\n",
    "dataset_dir = \"./GenAI-Hw5/Tang_training_data.json\"  # 設定資料集的目錄或檔案路徑\n",
    "logging_steps = 20  # 定義訓練過程中每隔多少步驟輸出一次訓練誌\n",
    "save_steps = 65  # 定義訓練過程中每隔多少步驟保存一次模型\n",
    "save_total_limit = 3  # 控制最多保留幾個模型checkpoint\n",
    "report_to = None  # 設定上報實驗指標的目標，預設為無\n",
    "MICRO_BATCH_SIZE = 4  # 定義微批次的大小\n",
    "BATCH_SIZE = 16  # 定義一個批次的大小\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE  # 計算每個微批次累積的梯度步數\n",
    "CUTOFF_LEN = 256  # 設定文本截斷的最大長度\n",
    "LORA_R = 8  # 設定LORA（Layer-wise Random Attention）的R值\n",
    "LORA_ALPHA = 16  # 設定LORA的Alpha值\n",
    "LORA_DROPOUT = 0.05  # 設定LORA的Dropout率\n",
    "VAL_SET_SIZE = 0  # 設定驗證集的大小，預設為無\n",
    "TARGET_MODULES = [\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"gate_proj\", \"v_proj\"] # 設定目標模組，這些模組的權重將被保存為checkpoint\n",
    "device_map = \"auto\"  # 設定設備映射，預設為\"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))  # 獲取環境變數\"WORLD_SIZE\"的值，若未設定則預設為1\n",
    "ddp = world_size != 1  # 根據world_size判斷是否使用分散式數據處理(DDP)，若world_size為1則不使用DDP\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output directory you specify\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "os.makedirs(ckpt_dir, exist_ok = True)\n",
    "\n",
    "# 根據 from_ckpt 標誌，從 checkpoint 載入模型權重\n",
    "if from_ckpt:\n",
    "    model = PeftModel.from_pretrained(model, ckpt_name)\n",
    "\n",
    "# 將模型準備好以使用 INT8 訓練\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# 使用 LoraConfig 配置 LORA 模型\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# 使用 Transformers Trainer 進行模型訓練\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=50,\n",
    "        num_train_epochs=num_epoch,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,  # 使用混合精度訓練\n",
    "        logging_steps=logging_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        output_dir=ckpt_dir,\n",
    "        save_total_limit=save_total_limit,\n",
    "        ddp_find_unused_parameters=False if ddp else None,  # 是否使用 DDP，控制梯度更新策略\n",
    "        report_to=report_to,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# 禁用模型的 cache 功能\n",
    "model.config.use_cache = False\n",
    "\n",
    "# 若使用 PyTorch 2.0 版本以上且非 Windows 系統，進行模型編譯\n",
    "if torch.__version__ >= \"2\" and sys.platform != 'win32':\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# 開始模型訓練\n",
    "trainer.train()\n",
    "\n",
    "# 將訓練完的模型保存到指定的目錄中\n",
    "model.save_pretrained(ckpt_dir)\n",
    "\n",
    "# 印出訓練過程中可能的缺失權重的警告信息\n",
    "print(\"\\n If there's a warning about missing keys above, please disregard :)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
